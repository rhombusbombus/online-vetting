{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import tiktoken\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "script_dir = os.path.dirname(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API Configuration\n",
    "MAX_TOKENS = 128000  # Max total tokens for gpt-4-0125-preview\n",
    "MAX_OUTPUT = 4096\n",
    "MAX_INPUT = MAX_TOKENS - MAX_OUTPUT\n",
    "TOKENS_PER_MINUTE_LIMIT = 70000  # TPM rate limit\n",
    "REQUESTS_PER_MINUTE_LIMIT = 470\n",
    "model = \"gpt-4-TPM-70k-RPM-420\"  # model = \"deployment_name\".\n",
    "input_cost = 0.01  # Per 1k tokens\n",
    "output_cost = 0.03  # Per 1k tokens\n",
    "\n",
    "\n",
    "# client = AzureOpenAI(\n",
    "#     api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "#     api_version=\"2024-02-15-preview\",\n",
    "#     azure_endpoint = os.getenv(\"AZURE_OPENAI_LANGUAGE_ENDPOINT\")\n",
    "#     )\n",
    "\n",
    "\n",
    "# response = client.chat.completions.create(\n",
    "#     model=\"gpt-4-turbo\", # model = \"deployment_name\".\n",
    "#     messages=[\n",
    "#         {\"role\": \"system\", \"content\": \"Assistant is a large language model trained by OpenAI.\"},\n",
    "#         {\"role\": \"user\", \"content\": \"Who were the founders of Microsoft?\"}\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# #print(response)\n",
    "# print(response.model_dump_json(indent=2))\n",
    "# print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(output_path, new_data):\n",
    "    temp_file_path = output_path.strip(\".json\")+\"_temp.json\"\n",
    "    with open(temp_file_path, 'w') as temp_file:\n",
    "        json.dump(new_data, temp_file, indent=4)\n",
    "\n",
    "    # Replace the old file with the new file\n",
    "    os.replace(temp_file_path, output_path)\n",
    "\n",
    "\n",
    "def get_websites():\n",
    "    filepath = os.path.join(script_dir, \"scraped_data\", \"company_data\", \"music_services2.csv\")\n",
    "    music_services = pd.read_csv(filepath)\n",
    "    music_services = music_services['music_services'].tolist()\n",
    "    return music_services\n",
    "\n",
    "\n",
    "def estimate_num_tokens_from_str(string, model=\"gpt-4-turbo\"):\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        print(\"Warning: Encoding not found. Using cl100k_base encoding.\")\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    #print(string)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "\n",
    "def get_cost(usage_obj, input_cost=0.0005, output_cost=0.0015):\n",
    "    return ((usage_obj.prompt_tokens/1000) * input_cost) + ((usage_obj.completion_tokens/1000) * output_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpt4turbo_summary(overwrite=False):\n",
    "\n",
    "    # Initializing variables\n",
    "    last_request_time = time.time()\n",
    "    total_tokens_this_session, tokens_this_minute, total_cost_this_session, requests_this_minute, total_requests_this_session = 0, 0, 0, 0, 0\n",
    "    json_path = os.path.join(script_dir, \"GPT_generated_data\", \"summary\", \"raw_data\", \"reddit_summary_keywords2.json\")\n",
    "\n",
    "    # Getting list of companies\n",
    "    url_list = get_websites() ################################## edit this\n",
    "    #url_list = [\"https://soundbetter.com/\"]\n",
    "    company_names = [re.search(r\"(?:www\\.)?(.*?)\\.\\w+$\", urlparse(url).netloc).group(1) for url in url_list]\n",
    "\n",
    "    # If JSON already exists and overwrite is FALSE, load file\n",
    "    if os.path.exists(json_path) and not overwrite:\n",
    "        with open(json_path, 'r') as file:\n",
    "            json_data = json.load(file)\n",
    "        skip_list = [entry.get(\"company\") for entry in json_data]  # Check which companies already have a summary\n",
    "        company_names = [company for company in company_names if company not in skip_list]\n",
    "    else:\n",
    "        json_data = []\n",
    "    \n",
    "    # for company in company_names:\n",
    "    #     # Import reddit data sets and remove irrelevant rows\n",
    "    #     submissions_filepath = os.path.join(script_dir, f\"scraped_data/reddit_data/submissions/train/{company}_train.csv\")\n",
    "    #     comments_filepath = os.path.join(script_dir, f\"scraped_data/reddit_data/comments/train/{company}_train.csv\")\n",
    "    #     if os.path.exists(submissions_filepath) and os.path.exists(comments_filepath):\n",
    "    #         sub_df = pd.read_csv(submissions_filepath, usecols = ['combined', 'gpt_filter2'], lineterminator='\\n')\n",
    "    #         sub_df = sub_df[sub_df[\"gpt_filter2\"] == True]\n",
    "    #         comment_df = pd.read_csv(comments_filepath, usecols = ['body_clean', 'gpt_filter2'], lineterminator='\\n')\n",
    "    #         comment_df = comment_df[comment_df[\"gpt_filter2\"] == True]\n",
    "    #     else:\n",
    "    #         print(\"DEBUG: continue\")\n",
    "    #         continue\n",
    "        \n",
    "    for company in company_names:\n",
    "        # Import reddit data sets and remove irrelevant rows\n",
    "        submissions_filepath = os.path.join(script_dir, f\"scraped_data/reddit_data/submissions/clean/{company}_clean.csv\")\n",
    "        comments_filepath = os.path.join(script_dir, f\"scraped_data/reddit_data/comments/clean/{company}_clean.csv\")\n",
    "        if os.path.exists(submissions_filepath) and os.path.exists(comments_filepath):\n",
    "            sub_df = pd.read_csv(submissions_filepath, lineterminator='\\n')\n",
    "            sub_df['combined'] = sub_df['title_clean'] + ': ' + sub_df['content_clean']\n",
    "            sub_df = sub_df[['combined']]\n",
    "            comment_df = pd.read_csv(comments_filepath, usecols = ['body_clean'], lineterminator='\\n')\n",
    "        else:\n",
    "            #print(\"DEBUG: continue\")\n",
    "            continue\n",
    "\n",
    "        # Combine all reddit posts and comments into a single list\n",
    "        posts = [post for post in sub_df['combined'].dropna()]\n",
    "        comments = [comment for comment in comment_df['body_clean'].dropna()]\n",
    "        reviews = posts + comments\n",
    "\n",
    "        summary = \"\"\n",
    "        result = {}\n",
    "        response = None\n",
    "        prompt = f\"\"\"\n",
    "        Analyze these Reddit posts and comments about a music PR/playlist promotion company named {company}. Generate a paragraph-long summary that focuses on customer opinions/concerns/experiences regarding {company}. Additionally, list out the most frequently mentioned aspects about the company, categorized as positive, negative, or neutral. Each aspect should be summarized in 1-2 words, ensuring that synonyms or similar variants are consolidated under a single term that best represents the sentiment expressed across mentions. For example, if \"high costs\" and \"expensive\" are used interchangeably but \"high costs\" is more common, use \"high costs\" for the negative aspects category. If an aspect could be interpreted in multiple ways (positive, negative, neutral), categorize it based on the overall sentiment it most commonly aligns with in the context of these reviews. Avoid listing the same aspect or closely related aspects (including synonyms or near-synonyms) in more than one category. Return a JSON object consisting of \"summary\", \"positive_aspects\", \"negative_aspects\", and \"neutral_aspects\".\n",
    "        \n",
    "        Please note:\n",
    "        - Keep in mind that some users may refer to multiple different services in the same post. Thus, only consider parts of the text that are explicitly referring to {company}. Ignore mentions of other services or irrelevant discussions.\n",
    "        - Do not mention other companies or services directly in your summary. \n",
    "        - Avoid fabricating information or introducing unrelated topics.\n",
    "\n",
    "        Example output (formatted as a valid JSON):\n",
    "        {{\n",
    "            \"summary\": \"Customers appreciate <company name> for its user-friendly platform, constructive feedback, relationship-building opportunities, organic stream growth, and playlist credibility. However, they raise concerns about high pricing, genre mismatches, limited reach for certain music types, and inconsistent campaign outcomes, including ineffective genre targeting and disappointing return on investment. Suggestions for improvement include refining the playlist matching process and enhancing the service to accommodate a broader range of music genres, aiming to increase successful playlist adds and exposure.\",\n",
    "            \"positive_aspects\": [\"User-friendly platform\", \"Constructive feedback\", \"Relationship-building\", \"Playlist credibility\"],\n",
    "            \"negative_aspects\": [\"Pricing\", \"Reach\", \"Campaign outcomes\", \"Genre targeting\", \"Engagement\", \"Return on investment\"],\n",
    "            \"neutral_aspects\": [\"Playlist placements\", \"Stream growth\", \"Exposure\"]\n",
    "        }}\n",
    "        \n",
    "        Here are the posts you will be analyzing: {reviews}\n",
    "\n",
    "        \\n```json\n",
    "        \"\"\"\n",
    "\n",
    "        # Calculating estimated number of tokens\n",
    "        estimate = estimate_num_tokens_from_str(prompt, model) + 7  # +1 for 'role', +6 for message primer\n",
    "        for review in reviews:\n",
    "            estimate += estimate_num_tokens_from_str(review, model)\n",
    "\n",
    "        # If estimated input tokens exceeds limit, throw warning\n",
    "        if estimate > MAX_INPUT:\n",
    "            raise ValueError(f\"WARNING: Estimated number of input tokens for {company} is {estimate}, which potentially exceeds the limit of {MAX_INPUT} tokens.\")\n",
    "\n",
    "        # Check if this request would exceed TPM or RPM limit\n",
    "        current_time = time.time()\n",
    "        if ((tokens_this_minute + estimate) > TOKENS_PER_MINUTE_LIMIT) or ((requests_this_minute + 1) > REQUESTS_PER_MINUTE_LIMIT):\n",
    "            sleep_time = 60 - (current_time - last_request_time) + 2  # 2 sec buffer\n",
    "            if sleep_time > 0:\n",
    "                time.sleep(sleep_time)\n",
    "            tokens_this_minute = 0  # Reset token count for the new minute\n",
    "            requests_this_minute = 0\n",
    "            last_request_time = time.time()  # Reset last request time\n",
    "\n",
    "        # Generate summary\n",
    "        client = AzureOpenAI(\n",
    "        api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "        api_version=\"2024-02-15-preview\",\n",
    "        azure_endpoint = os.getenv(\"AZURE_OPENAI_LANGUAGE_ENDPOINT\")\n",
    "        )\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt + f\"{reviews}\"}],\n",
    "                max_tokens=MAX_OUTPUT,\n",
    "                stop=None,\n",
    "                n=1,\n",
    "                response_format= {\n",
    "                    \"type\": \"json_object\"\n",
    "                    }\n",
    "            )\n",
    "        \n",
    "\n",
    "        if response:\n",
    "            generated_json = response.choices[0].message.content\n",
    "            print(generated_json)\n",
    "            generated_json = json.loads(generated_json)\n",
    "            generated_summary = generated_json['summary']\n",
    "            generated_positive_keywords = generated_json['positive_aspects']\n",
    "            generated_negative_keywords = generated_json['negative_aspects']\n",
    "            generated_neutral_keywords = generated_json['neutral_aspects']\n",
    "            summary = generated_summary if generated_summary else summary\n",
    "\n",
    "            tokens_this_minute += response.usage.total_tokens\n",
    "            total_tokens_this_session += tokens_this_minute\n",
    "            total_cost_this_session += get_cost(response.usage, input_cost, output_cost)\n",
    "            requests_this_minute += 1\n",
    "            total_requests_this_session += 1\n",
    "\n",
    "        # Save response object as JSON\n",
    "        if response:\n",
    "            result = {\n",
    "                \"summary_id\": response.id,\n",
    "                \"model\": response.model,\n",
    "                \"created\": response.created,\n",
    "                \"total_tokens_used\": response.usage.total_tokens,\n",
    "                \"cost\": get_cost(response.usage, input_cost, output_cost),\n",
    "                \"company\": company,\n",
    "                \"summary\": summary,\n",
    "                \"positive_keywords\": generated_positive_keywords,\n",
    "                \"negative_keywords\": generated_negative_keywords,\n",
    "                \"neutral_keywords\": generated_neutral_keywords,\n",
    "            }\n",
    "            json_data.append(result)\n",
    "            save_json(json_path, json_data)\n",
    "\n",
    "            print(f\"####### SUMMARY FOR {company} #######\")\n",
    "            print(summary)\n",
    "            print(\"####### DEBUG PURPOSES #######\")\n",
    "            print(f\"Estimated input tokens used: {estimate}\") \n",
    "            print(f\"Actual input tokens used: {response.usage.prompt_tokens}\")\n",
    "            print(\"####### CURRENT USAGE #######\")\n",
    "            print(f\"Company: {company}\")\n",
    "            print(f\"Tokens used for this summary: {response.usage.total_tokens}\")\n",
    "            print(f\"Cost of this summary: {get_cost(response.usage, input_cost, output_cost)}\")\n",
    "            print(\"####### SESSION STATS #######\")\n",
    "            print(f\"Total tokens used so far: {total_tokens_this_session}\")\n",
    "            print(f\"Total requests so far: {total_requests_this_session}\")\n",
    "            print(f\"Total cost so far: {total_cost_this_session}\")\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"summary\": \"Broadjam is a mixed bag for its users, who express varied experiences and opinions. Some users find it to be a useful platform for submitting music to sync license opportunities and have achieved successful placements. There are mentions of utilizing Broadjam to connect with libraries, industry professionals, and even securing deals with TV shows like CSI. However, skepticism about the site's legitimacy is evident, with users referring to it as 'broadscam' and criticizing it for high submission costs without guaranteed follow-ups or placements. The interface is described as dated, and while it offers free song postings, the strategies for exposure are perceived as lackluster compared to other services. Overall, the sentiment leans towards a cautious approach when utilizing Broadjam, with a recommendation to explore more reputable publishers or directly approach music supervisors for better chances of success.\",\n",
      "    \"positive_aspects\": [\"Successful placements\", \"Industry connections\", \"Learning resources\"],\n",
      "    \"negative_aspects\": [\"High submission costs\", \"Interface\", \"Exposure strategies\", \"Placement guarantee\", \"Professional feedback\"],\n",
      "    \"neutral_aspects\": [\"Community feedback\", \"Song posting\", \"Profile presence\"]\n",
      "}\n",
      "####### SUMMARY FOR broadjam #######\n",
      "Broadjam is a mixed bag for its users, who express varied experiences and opinions. Some users find it to be a useful platform for submitting music to sync license opportunities and have achieved successful placements. There are mentions of utilizing Broadjam to connect with libraries, industry professionals, and even securing deals with TV shows like CSI. However, skepticism about the site's legitimacy is evident, with users referring to it as 'broadscam' and criticizing it for high submission costs without guaranteed follow-ups or placements. The interface is described as dated, and while it offers free song postings, the strategies for exposure are perceived as lackluster compared to other services. Overall, the sentiment leans towards a cautious approach when utilizing Broadjam, with a recommendation to explore more reputable publishers or directly approach music supervisors for better chances of success.\n",
      "####### DEBUG PURPOSES #######\n",
      "Estimated input tokens used: 38723\n",
      "Actual input tokens used: 38939\n",
      "####### CURRENT USAGE #######\n",
      "Company: broadjam\n",
      "Tokens used for this summary: 39173\n",
      "Cost of this summary: 0.39641000000000004\n",
      "####### SESSION STATS #######\n",
      "Total tokens used so far: 39173\n",
      "Total requests so far: 1\n",
      "Total cost so far: 0.39641000000000004\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "WARNING: Estimated number of input tokens for kwork is 329631, which potentially exceeds the limit of 123904 tokens.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m get_gpt4turbo_summary()\n",
      "Cell \u001b[0;32mIn[20], line 84\u001b[0m, in \u001b[0;36mget_gpt4turbo_summary\u001b[0;34m(overwrite)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[39m# If estimated input tokens exceeds limit, throw warning\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[39mif\u001b[39;00m estimate \u001b[39m>\u001b[39m MAX_INPUT:\n\u001b[0;32m---> 84\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWARNING: Estimated number of input tokens for \u001b[39m\u001b[39m{\u001b[39;00mcompany\u001b[39m}\u001b[39;00m\u001b[39m is \u001b[39m\u001b[39m{\u001b[39;00mestimate\u001b[39m}\u001b[39;00m\u001b[39m, which potentially exceeds the limit of \u001b[39m\u001b[39m{\u001b[39;00mMAX_INPUT\u001b[39m}\u001b[39;00m\u001b[39m tokens.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     86\u001b[0m \u001b[39m# Check if this request would exceed TPM or RPM limit\u001b[39;00m\n\u001b[1;32m     87\u001b[0m current_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "\u001b[0;31mValueError\u001b[0m: WARNING: Estimated number of input tokens for kwork is 329631, which potentially exceeds the limit of 123904 tokens."
     ]
    }
   ],
   "source": [
    "get_gpt4turbo_summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "papaya",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
