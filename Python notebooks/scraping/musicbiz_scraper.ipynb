{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3386b2c8-d3b0-442d-8b23-8a5bf67d7d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import phonenumbers\n",
    "import asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "from requests_html import AsyncHTMLSession\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "logging.getLogger('scrapy').propagate = False\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9bf8fc5c-3fcd-48b2-a47e-a5963513d6fe",
   "metadata": {},
   "source": [
    "## Scraping company website links from musicbiz.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d177d3f3-c022-40a0-b18d-fd0c449be434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get absolute path of script\n",
    "#script_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))   #Use for .py script only\n",
    "script_dir = os.path.dirname(os.getcwd())   #Use for python notebook only\n",
    "\n",
    "headers = {\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "    'accept-language': 'de,de-DE;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6,fr;q=0.5,de-CH;q=0.4,es;q=0.3',\n",
    "    'cache-control': 'no-cache',\n",
    "    'dnt': '1',\n",
    "    'pragma': 'no-cache',\n",
    "    'sec-ch-ua': '\"Not_A Brand\";v=\"99\", \"Microsoft Edge\";v=\"109\", \"Chromium\";v=\"109\"',\n",
    "    'sec-ch-ua-arch': '\"x86\"',\n",
    "    'sec-ch-ua-bitness': '\"64\"',\n",
    "    'sec-ch-ua-full-version': '\"109.0.1518.78\"',\n",
    "    'sec-ch-ua-full-version-list': '\"Not_A Brand\";v=\"99.0.0.0\", \"Microsoft Edge\";v=\"109.0.1518.78\", \"Chromium\";v=\"109.0.5414.120\"',\n",
    "    'sec-ch-ua-mobile': '?0',\n",
    "    'sec-ch-ua-model': '\"\"',\n",
    "    'sec-ch-ua-platform': '\"Windows\"',\n",
    "    'sec-ch-ua-platform-version': '\"10.0.0\"',\n",
    "    'sec-ch-ua-wow64': '?0',\n",
    "    'sec-fetch-dest': 'document',\n",
    "    'sec-fetch-mode': 'navigate',\n",
    "    'sec-fetch-site': 'none',\n",
    "    'sec-fetch-user': '?1',\n",
    "    'upgrade-insecure-requests': '1',\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36 Edg/109.0.1518.78',\n",
    "}\n",
    "\n",
    "google_headers = {\n",
    "    'authority': 'www.google.com',\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "    'accept-language': 'de,de-DE;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6,fr;q=0.5,de-CH;q=0.4,es;q=0.3',\n",
    "    'cache-control': 'no-cache',\n",
    "    'dnt': '1',\n",
    "    'pragma': 'no-cache',\n",
    "    'sec-ch-ua': '\"Not_A Brand\";v=\"99\", \"Microsoft Edge\";v=\"109\", \"Chromium\";v=\"109\"',\n",
    "    'sec-ch-ua-arch': '\"x86\"',\n",
    "    'sec-ch-ua-bitness': '\"64\"',\n",
    "    'sec-ch-ua-full-version': '\"109.0.1518.78\"',\n",
    "    'sec-ch-ua-full-version-list': '\"Not_A Brand\";v=\"99.0.0.0\", \"Microsoft Edge\";v=\"109.0.1518.78\", \"Chromium\";v=\"109.0.5414.120\"',\n",
    "    'sec-ch-ua-mobile': '?0',\n",
    "    'sec-ch-ua-model': '\"\"',\n",
    "    'sec-ch-ua-platform': '\"Windows\"',\n",
    "    'sec-ch-ua-platform-version': '\"10.0.0\"',\n",
    "    'sec-ch-ua-wow64': '?0',\n",
    "    'sec-fetch-dest': 'document',\n",
    "    'sec-fetch-mode': 'navigate',\n",
    "    'sec-fetch-site': 'none',\n",
    "    'sec-fetch-user': '?1',\n",
    "    'upgrade-insecure-requests': '1',\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36 Edg/109.0.1518.78',\n",
    "}\n",
    "\n",
    "musicbiz_url = 'https://musicbiz.org/about/member-community/#member-companies'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "62dfabf6",
   "metadata": {},
   "source": [
    "### General helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a0af621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_website(url):\n",
    "    \"\"\"Fetches HTML content of site and returns it as a BeautifulSoup object.\n",
    "    \"\"\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    html_text = response.content\n",
    "    soup = BeautifulSoup(html_text, 'lxml')\n",
    "    return soup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a993c658-e636-4c73-a633-2090f68c63c9",
   "metadata": {},
   "source": [
    "## Checking TrustPilot for contact info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "087043fa-0c9b-4058-8485-92fdf2b353f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trustpilot_contact_scraper(url):\n",
    "    domain = re.findall(r'https?:\\/\\/(?:www\\.)?([a-zA-Z0-9.-]+)',  url)[0]\n",
    "    trustpilot_page = f'https://www.trustpilot.com/review/{domain}'\n",
    "    soup = get_website(trustpilot_page)\n",
    "\n",
    "    # Checking if profile exists or if we land on a 404 page\n",
    "    if soup.find('div', class_=\"errors_error404__tUqzU\"):\n",
    "        return None, None, None  # Return None if profile doesn't exist\n",
    "\n",
    "    email, phone, address = '', '', ''\n",
    "\n",
    "    ul_tag = soup.find('ul', class_=\"styles_contactInfoElements__YqQAJ\") # Tag for review card\n",
    "\n",
    "    if ul_tag:\n",
    "        # Find the <a> tag for email\n",
    "        a_tag = ul_tag.find('a', href=lambda href: href and 'mailto:' in href)\n",
    "        if a_tag and 'mailto:' in a_tag['href']:\n",
    "            email = a_tag['href'].replace('mailto:', '')\n",
    "\n",
    "        # Find the <li> tag for phone\n",
    "        li_tag = ul_tag.find('li', class_=\"styles_contactInfoElement__SxlS3\")\n",
    "        if li_tag:\n",
    "            a_tag2 = li_tag.find('a', href=lambda href: href and 'tel:' in href)\n",
    "            if a_tag2 and 'tel:' in a_tag2['href']:\n",
    "                phone = a_tag2['href'].replace('tel:', '')\n",
    "                \n",
    "\n",
    "        # Find the <li> tag for address\n",
    "        ul_tag2 = ul_tag.find('ul', class_=\"typography_body-m__xgxZ_ typography_appearance-default__AAY17 styles_contactInfoAddressList__RxiJI\")\n",
    "        if ul_tag2:\n",
    "            address_lines = [li.text for li in ul_tag2.find_all('li')]\n",
    "            address = ', '.join(address_lines)\n",
    "\n",
    "    return email, phone, address"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9af3b1a5",
   "metadata": {},
   "source": [
    "## Scraping Google local business listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64b7aec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def google_contact_scraper(url):\n",
    "    name = re.findall(r'[\\w]+://(?:www.)?(.+?).[\\w]+/',  url)[0]\n",
    "    \n",
    "    params = {\n",
    "        'q': name,\n",
    "        'hl': 'en'\n",
    "    }\n",
    "\n",
    "    phone, address = '', ''\n",
    "\n",
    "    response = requests.get('https://www.google.com/search', params=params, headers=google_headers)\n",
    "    html_text = response.content\n",
    "    soup = str(BeautifulSoup(html_text, 'lxml'))\n",
    "\n",
    "    address_regex = '<span class=\"LrzXr\">(.*?)<\\/span>'\n",
    "    phone_regex = '(?:<span aria-label[^>]+?)>([0-9()+\\-\\s]+)(?:<\\/span>)'\n",
    "    \n",
    "    address = re.findall(address_regex, soup) \n",
    "    phone = re.findall(phone_regex, soup) \n",
    "        \n",
    "    return phone, address"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa2a3480-a5ab-462b-b969-da85de2da146",
   "metadata": {},
   "source": [
    "## Scraping company website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c586135",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrape_url(url):\n",
    "    soup = get_website(url)\n",
    "    links = [a['href'] for div in soup.find_all('div', class_='sponsor-logo-wrapper') for a in div.find_all('a', href=True)]\n",
    "    return links\n",
    "\n",
    "\n",
    "def scrape_url_batch():\n",
    "    links = scrape_url(url)\n",
    "\n",
    "    file_path = os.path.join(script_dir, \"scraping\", \"scraped_data\", \"company_data\", \"musicbiz_data.csv\")\n",
    "\n",
    "    with open(file_path, 'w', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(['url'])\n",
    "        for item in links:\n",
    "            csvwriter.writerow([item])\n",
    "\n",
    "    return links\n",
    "\n",
    "\n",
    "def extract_soup(soup):\n",
    "    soup_str = str(soup)\n",
    "\n",
    "    # Extract all links on the page\n",
    "    a_tags = list(soup.find_all(\"a\"))\n",
    "    extracted_links = [a_tag.get('href') for a_tag in a_tags if a_tag.get('href')] \n",
    "    extracted_links = list(set(extracted_links))\n",
    "\n",
    "    # Extract emails\n",
    "    email_regex = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9-]+\\.[A-Z|a-z]{2,}(?![\\d.])\\b'\n",
    "    #emails_in_tags = [re.findall(email_regex, link) for link in extracted_links]\n",
    "    #emails_in_tags = sum(emails_in_tags, [])\n",
    "    emails = re.findall(email_regex, str(soup))\n",
    "    emails = list(set(emails))\n",
    "\n",
    "    # Extract phone numbers\n",
    "    phone_regex = '\\\"tel\\:\\+*([\\(\\)\\-0-9\\ ]{1,})\\\"'\n",
    "    phone_regex2 = '(?:\\+1[\\s]?)?(?:1[\\s-]?)?\\(?\\d{3}\\)?[\\s-]\\d{3}[\\s-]\\d{4}'\n",
    "    phone_matches = re.findall(phone_regex, soup_str) \n",
    "    phone_matches2 = re.findall(phone_regex2, soup_str)\n",
    "    phones = list(set(phone_matches + phone_matches2))\n",
    "\n",
    "    return emails, phones\n",
    "\n",
    "\n",
    "def site_scraper(url): \n",
    "    # Extract the HTML \n",
    "    print(f\"Currently scraping: {url}\")\n",
    "    soup = get_website(url)\n",
    "    emails, phones = extract_soup(soup)\n",
    "    return emails, phones\n",
    "\n",
    "\n",
    "async def get_website_async(url):\n",
    "    \"\"\" Fetch HTML content of a website, including dynamically loaded JavaScript content.\n",
    "    \"\"\"\n",
    "    asession = AsyncHTMLSession() \n",
    "    r = await asession.get(url, headers=headers) \n",
    "    await r.html.arender(timeout=30) # Wait max 20 sec for JavaScript to render\n",
    "    html = r.html.raw_html \n",
    "    await asession.close()\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    return soup\n",
    "\n",
    "\n",
    "async def site_scraper_async(url): \n",
    "    print(f\"Currently scraping dynamic HTML: {url}\")\n",
    "    soup = await get_website_async(url)\n",
    "    emails, phones = extract_soup(soup)\n",
    "    return emails, phones\n",
    "\n",
    "\n",
    "def get_first_level_directories(url):\n",
    "    soup = get_website(url)\n",
    "    links = set()\n",
    "    domain = urlparse(url).netloc.replace('www.', '', 1)\n",
    "\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        full_link = urljoin(url, link['href'])  # Resolve relative URLs\n",
    "        parsed_link = urlparse(full_link)\n",
    "        path_segments = [segment for segment in parsed_link.path.split('/') if segment]\n",
    "\n",
    "        # Check if the link is a first sub-level directory\n",
    "        parsed_domain = parsed_link.netloc.replace('www.', '', 1)\n",
    "        if (parsed_domain == domain) and (len(path_segments) == 1):\n",
    "            links.add(f\"{parsed_link.scheme}://{parsed_domain}/{path_segments[0]}\")\n",
    "\n",
    "    return links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bc18067-9576-4f88-a7e0-ad4ba5bbbb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def main(urls):\n",
    "    email_dict, phone_dict, address_dict = {}, {}, {}\n",
    "    keywords = [\"contact\", \"about\", \"faq\", \"help\", \"privacy\", \"terms\"]\n",
    "    \n",
    "    for url in urls:\n",
    "        # Scraping TrustPilot profile\n",
    "        emails_list, phones_list, address_list = trustpilot_contact_scraper(url)\n",
    "        emails_list = [emails_list]\n",
    "        phones_list = [phones_list]\n",
    "        address_list = [address_list]\n",
    "        email_dict.update({url:emails_list})\n",
    "        phone_dict.update({url:phones_list})\n",
    "        address_dict.update({url:address_list})\n",
    "\n",
    "        # Scraping Google listings\n",
    "        phones_list, address_list = google_contact_scraper(url)\n",
    "        if phone_dict[url]:\n",
    "            phone_dict[url].extend(phones_list)\n",
    "        else:\n",
    "            phone_dict[url] = phones_list\n",
    "        if address_dict[url]:\n",
    "            address_dict[url].extend(address_list)\n",
    "        else:\n",
    "            address_dict[url] = address_list\n",
    "\n",
    "        # Scraping the root directory\n",
    "        emails_list, phones_list = site_scraper(url)\n",
    "        if phone_dict[url]:\n",
    "            phone_dict[url].extend(phones_list)\n",
    "        else:\n",
    "            phone_dict[url] = phones_list\n",
    "        if email_dict[url]:\n",
    "            email_dict[url].extend(emails_list)\n",
    "        else:\n",
    "            email_dict[url] = emails_list\n",
    "        \n",
    "        # Scraping the sub directories\n",
    "        if all(x is None or x == \"\" for x in email_dict[url]):\n",
    "            subdirs = get_first_level_directories(url)\n",
    "            subdirs = [link for link in subdirs if any(keyword in link for keyword in keywords)]\n",
    "            for subdir in subdirs:\n",
    "                if not all(x is None or x == \"\" for x in email_dict[url]):\n",
    "                    break\n",
    "                emails_list, phones_list = site_scraper(subdir)\n",
    "                if phone_dict[url]:\n",
    "                    phone_dict[url].extend(phones_list)\n",
    "                else:\n",
    "                    phone_dict[url] = phones_list\n",
    "                if email_dict[url]:\n",
    "                    email_dict[url].extend(emails_list)\n",
    "                else:\n",
    "                    email_dict[url] = emails_list\n",
    "\n",
    "        # If no emails found, fallback to slower scraping method\n",
    "        if all(x is None or x == \"\" for x in email_dict[url]):\n",
    "            emails_list, phones_list = await site_scraper_async(url)\n",
    "            if phone_dict[url]:\n",
    "                phone_dict[url].extend(phones_list)\n",
    "            else:\n",
    "                phone_dict[url] = phones_list\n",
    "            if email_dict[url]:\n",
    "                email_dict[url].extend(emails_list)\n",
    "            else:\n",
    "                email_dict[url] = emails_list\n",
    "            \n",
    "\n",
    "    new_email_dict = {k: [v] if v else [np.nan] for k, v in email_dict.items()}\n",
    "    new_phone_dict = {k: [v] if v else [np.nan] for k, v in phone_dict.items()}\n",
    "    new_address_dict = {k: [v] if v else [np.nan] for k, v in address_dict.items()}\n",
    "\n",
    "    email_df = pd.DataFrame.from_dict(new_email_dict, orient='index', columns=['emails'])\n",
    "    phone_df = pd.DataFrame.from_dict(new_phone_dict, orient='index', columns=['phone numbers'])\n",
    "    address_df = pd.DataFrame.from_dict(new_address_dict, orient='index', columns=['addresses'])\n",
    "\n",
    "    # Cleaning and formatting\n",
    "    email_df.loc[:, 'emails'] = email_df['emails'].apply(lambda email_list: [item for item in email_list if item and item.strip()])\n",
    "    email_df.loc[:, 'emails'] = email_df['emails'].apply(lambda email_list: list(set(email_list)))\n",
    "\n",
    "    phone_df.loc[:, 'phone numbers'] = phone_df['phone numbers'].apply(lambda phone_list: [item for item in phone_list if item and item.strip()])\n",
    "    phone_df.loc[:, 'phone numbers'] = phone_df['phone numbers'].apply(lambda phone_list: [phonenumbers.format_number(phonenumbers.parse(number, 'US'), phonenumbers.PhoneNumberFormat.E164)for number in phone_list])\n",
    "    phone_df.loc[:, 'phone numbers'] = phone_df['phone numbers'].apply(lambda phone_list: list(set(phone_list)))\n",
    "\n",
    "    address_df.loc[:, 'addresses'] = address_df['addresses'].apply(lambda address_list: [item for item in address_list if item and item.strip()])\n",
    "    address_df.loc[:, 'addresses'] = address_df['addresses'].apply(lambda address_list: list(set(address_list)))\n",
    "    return email_df.join(phone_df).join(address_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93a3fa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "music_services = pd.read_csv(filepath)\n",
    "music_services = music_services['music_services'].tolist()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa50d056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently scraping: https://abkco.com/\n",
      "Currently scraping: https://abkco.com/about\n",
      "Currently scraping: https://abkco.com/privacy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emails</th>\n",
       "      <th>phone numbers</th>\n",
       "      <th>addresses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>https://abkco.com/</th>\n",
       "      <td>[info@abkco.com, unsubscribe@abkco.com]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     emails phone numbers  \\\n",
       "https://abkco.com/  [info@abkco.com, unsubscribe@abkco.com]            []   \n",
       "\n",
       "                   addresses  \n",
       "https://abkco.com/        []  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await main([\"https://abkco.com/\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ff57ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(script_dir, \"scraping\", \"scraped_data\", \"company_data\", \"musicbiz_sites.csv\")\n",
    "url_list = []\n",
    "with open(filepath, mode='r', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader, None)  # Skip header\n",
    "    for row in reader:\n",
    "        url_list.append(str(row[0]))\n",
    "\n",
    "def extract_domain_name_compact(url):\n",
    "    return re.search(r\"(?:www\\.)?(.*?)\\.\\w+$\", urlparse(url).netloc).group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2be09c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.1001tracklists.com/\n",
      "1001tracklists\n",
      "https://1990records.org/\n",
      "1990records\n",
      "https://www.615jjentertainment.com/\n",
      "615jjentertainment\n",
      "http://www.ascap.com/\n",
      "ascap\n",
      "https://www.atozmedia.com/\n",
      "atozmedia\n",
      "https://www.abkco.com/\n",
      "abkco\n",
      "http://www.ada-music.com\n",
      "ada-music\n",
      "http://www.adargagroup.com/\n",
      "adargagroup\n",
      "https://www.advamobile.com/public/default.aspx\n",
      "advamobile\n",
      "https://www.afmsagaftrafund.org/\n",
      "afmsagaftrafund\n",
      "https://www.aimsapi.com/\n",
      "aimsapi\n",
      "https://www.allmediasupply.com/\n",
      "allmediasupply\n",
      "https://www.aent.com/\n",
      "aent\n",
      "https://thealliancerocks.com/\n",
      "thealliancerocks\n",
      "https://www.amazon.com/\n",
      "amazon\n",
      "http://www.ampeddistribution.com/\n",
      "ampeddistribution\n",
      "https://www.amuse.io/en/\n",
      "amuse\n",
      "http://www.angrymobmusic.com/\n",
      "angrymobmusic\n",
      "https://www.apple.com/music/\n",
      "apple\n",
      "https://www.aristarecordings.com/\n",
      "aristarecordings\n",
      "https://www.arscounsel.com/\n",
      "arscounsel\n",
      "https://artistgrowth.com/\n",
      "artistgrowth\n",
      "http://www.atlanticrecords.com/\n",
      "atlanticrecords\n",
      "https://www.audiam.com/\n",
      "audiam\n",
      "https://audius.co/\n",
      "audius\n",
      "https://averagejoesent.com/\n",
      "averagejoesent\n",
      "https://www.awal.com/?utm_campaign=brand&utm_source=google&utm_medium=cpc&utm_campaign=%5BS%5D+-+AWAL+Brand&utm_source=adwords&utm_term=%2Bawal&utm_medium=ppc&hsa_kw=%2Bawal&hsa_src=g&hsa_ver=3&hsa_ad=278691736307&hsa_acc=2473481215&hsa_mt=b&hsa_net=adwords&hsa_tgt=kwd-301453869273&hsa_grp=53619099577&hsa_cam=1409899615&gclid=EAIaIQobChMIiZbUo7yJ6QIVCVYMCh0szA_KEAAYASAAEgJXCfD_BwE\n",
      "awal\n",
      "http://www.backstagemusica.com/\n",
      "backstagemusica\n",
      "https://bandcamp.com/\n",
      "bandcamp\n",
      "https://bandlabtechnologies.com/\n",
      "bandlabtechnologies\n",
      "https://barefootllc.com/teams/bill-campbell/\n",
      "barefootllc\n",
      "https://bark-entertainment.com/\n",
      "bark-entertainment\n",
      "https://beatbread.com/\n",
      "beatbread\n",
      "https://beatdapp.com/\n",
      "beatdapp\n",
      "https://www.beatsbydre.com/\n",
      "beatsbydre\n",
      "https://www.beggars.com/\n",
      "beggars\n",
      "https://www.bigmachinelabelgroup.com\n",
      "bigmachinelabelgroup\n",
      "http://www.bigyellowdogmusic.com\n",
      "bigyellowdogmusic\n",
      "https://www.bittersweet.media/\n",
      "bittersweet\n",
      "https://www.blackriverent.com/\n",
      "blackriverent\n",
      "https://www.bmat.com/\n",
      "bmat\n",
      "https://www.bmg.com/de/\n",
      "bmg\n",
      "https://www.bmi.com/\n",
      "bmi\n",
      "https://boomy.com/\n",
      "boomy\n",
      "https://www.bravado.com/\n",
      "bravado\n",
      "https://www.brightantenna.com/\n",
      "brightantenna\n",
      "http://www.capitolchristianmusicgroup.com/\n",
      "capitolchristianmusicgroup\n",
      "http://www.capitolcmgpublishing.com\n",
      "capitolcmgpublishing\n",
      "https://www.capitolmusicgroup.com\n",
      "capitolmusicgroup\n",
      "https://www.cashear.com/\n",
      "cashear\n",
      "https://www.catchpointrights.com/\n",
      "catchpointrights\n",
      "https://cdbaby.com\n",
      "cdbaby\n",
      "https://chartmetric.io/landing\n",
      "chartmetric\n",
      "http://www.cimsmusic.com/\n",
      "cimsmusic\n",
      "https://cinqmusic.com/\n",
      "cinqmusic\n",
      "http://cleorecs.com/home/\n",
      "cleorecs\n",
      "https://www.clicknclear.com/\n",
      "clicknclear\n",
      "https://www.cmhrecords.com\n",
      "cmhrecords\n",
      "https://www.cmrra.ca/\n",
      "cmrra\n",
      "http://www.nceg.org/\n",
      "nceg\n",
      "http://www.columbiarecords.com\n",
      "columbiarecords\n",
      "https://compassrecords.com/\n",
      "compassrecords\n",
      "https://compoundinterestentertainment.com/\n",
      "compoundinterestentertainment\n",
      "https://concord.com\n",
      "concord\n",
      "https://concordjazz.com/\n",
      "concordjazz\n",
      "https://concord.com/music-publishing/\n",
      "concord\n",
      "https://www.concordrecords.com/\n",
      "concordrecords\n",
      "https://www.cosynd.com/\n",
      "cosynd\n",
      "https://www.counselllp.com/\n",
      "counselllp\n",
      "https://cpidistribution.com/\n",
      "cpidistribution\n",
      "https://www.crossborderworks.com/\n",
      "crossborderworks\n",
      "http://crowdsurf.net/\n",
      "crowdsurf\n",
      "https://craftrecordings.com/\n",
      "craftrecordings\n",
      "https://www.curb.com\n",
      "curb\n",
      "https://www.curveroyaltysystems.com\n",
      "curveroyaltysystems\n",
      "https://daaci.com/\n",
      "daaci\n",
      "https://dangerbirdrecords.com/\n",
      "dangerbirdrecords\n",
      "http://www.dare-records.com/\n",
      "dare-records\n",
      "https://dbssounds.com/\n",
      "dbssounds\n",
      "https://decca.com\n",
      "decca\n",
      "https://www.defjam.com\n",
      "defjam\n",
      "https://deptofrecordstores.com\n",
      "deptofrecordstores\n",
      "https://get.hiphop\n",
      "get\n",
      "https://www.dpgworldwide.com/\n",
      "dpgworldwide\n",
      "https://www.discogs.com/\n",
      "discogs\n",
      "https://www.disneyconcerts.com/\n",
      "disneyconcerts\n",
      "https://music.disney.com/\n",
      "music.disney\n",
      "https://disneymusicpublishing.com/\n",
      "disneymusicpublishing\n",
      "https://www.distro.direct/\n",
      "distro\n",
      "https://distrokid.com/\n",
      "distrokid\n",
      "https://www.dmgclearances.com/\n",
      "dmgclearances\n",
      "https://www.aent.com/\n",
      "aent\n",
      "https://www.dolby.com/\n",
      "dolby\n",
      "https://www.downtownmusic.com/\n",
      "downtownmusic\n",
      "https://dtlrradio.com/\n",
      "dtlrradio\n",
      "https://www.dualtone.com/\n",
      "dualtone\n",
      "https://www.dynamictalentint.com/\n",
      "dynamictalentint\n",
      "http://www.elektrarecords.com\n",
      "elektrarecords\n",
      "https://live.eluv.io/\n",
      "live.eluv\n",
      "https://www.empi.re/\n",
      "empi\n",
      "https://www.entergain.com/\n",
      "entergain\n",
      "https://entertainment-intelligence.com/\n",
      "entertainment-intelligence\n",
      "https://www.epicrecords.com/\n",
      "epicrecords\n",
      "http://epitaph.com/\n",
      "epitaph\n",
      "https://www.equitydistro.com/\n",
      "equitydistro\n",
      "https://www.excelerationmusic.com/\n",
      "excelerationmusic\n",
      "http://www.exclusivecompany.com\n",
      "exclusivecompany\n",
      "https://exploration.io/\n",
      "exploration\n",
      "https://famehouse.net\n",
      "famehouse\n",
      "http://www.fanmailmarketing.com/\n",
      "fanmailmarketing\n",
      "https://fantasyrecordings.com/\n",
      "fantasyrecordings\n",
      "https://fearlessrecords.com/\n",
      "fearlessrecords\n",
      "https://feature.fm/\n",
      "feature\n",
      "http://feed.fm/\n",
      "feed\n",
      "https://platform.flymachine.com/events\n",
      "platform.flymachine\n",
      "https://www.famscoalition.com/\n",
      "famscoalition\n",
      "https://flatironrecordings.com/\n",
      "flatironrecordings\n",
      "https://fonico.mobi/\n",
      "fonico\n",
      "https://www.freshnsassy.com/\n",
      "freshnsassy\n",
      "http://fuga.com\n",
      "fuga\n",
      "https://galleryofsound.com\n",
      "galleryofsound\n",
      "https://www.galorentertainment.com/\n",
      "galorentertainment\n",
      "https://giggs.live/\n",
      "giggs\n",
      "https://godigitalmg.com/\n",
      "godigitalmg\n",
      "https://www.gogoods.io/about\n",
      "gogoods\n",
      "https://goldstate.com/\n",
      "goldstate\n",
      "https://www.gracenote.com/\n",
      "gracenote\n",
      "https://gdrfirm.com/\n",
      "gdrfirm\n",
      "https://www.gsbmusic.com/\n",
      "gsbmusic\n",
      "https://guinrecords.com/\n",
      "guinrecords\n",
      "https://www.harryfox.com/#/\n",
      "harryfox\n",
      "https://www.heartdancerecords.com/\n",
      "heartdancerecords\n",
      "https://hi.fi/\n",
      "hi\n",
      "https://www.hitskope.com/\n",
      "hitskope\n",
      "https://www.hollywoodrecords.com/\n",
      "hollywoodrecords\n",
      "https://www.hopelessrecords.com/ https://www.hopelessrecords.com/aboutsubcity/\n",
      "hopelessrecords\n",
      "http://www.iceservices.com\n",
      "iceservices\n",
      "https://www.independent.co/\n",
      "independent\n",
      "https://indiefy.net/\n",
      "indiefy\n",
      "https://infiniteaggregate.com/\n",
      "infiniteaggregate\n",
      "https://www.ingramentertainment.com/\n",
      "ingramentertainment\n",
      "http://www.ingrooves.com/\n",
      "ingrooves\n",
      "https://www.innercatmusic.com/\n",
      "innercatmusic\n",
      "https://www.instagram.com/\n",
      "instagram\n",
      "https://www.collectmypub.com/\n",
      "collectmypub\n",
      "https://www.ircamamplify.com/\n",
      "ircamamplify\n",
      "http://www.islandrecords.com/\n",
      "islandrecords\n",
      "https://www.jambase.com/\n",
      "jambase\n",
      "https://jewelboxplatinum.com/\n",
      "jewelboxplatinum\n",
      "http://jullianrecords.com/home\n",
      "jullianrecords\n",
      "https://direct.killphonicrights.com/\n",
      "direct.killphonicrights\n",
      "https://killrockstars.com/\n",
      "killrockstars\n",
      "https://www.labelmiraclestudioapps.com/\n",
      "labelmiraclestudioapps\n",
      "https://www.label-logic.net/\n",
      "label-logic\n",
      "https://www.landr.com/en/\n",
      "landr\n",
      "https://lastgang.com/\n",
      "lastgang\n",
      "https://www.legacyrecordings.com/\n",
      "legacyrecordings\n",
      "https://www.lovemorerecords.com/\n",
      "lovemorerecords\n",
      "http://www.lullify.com/\n",
      "lullify\n",
      "https://luminatedata.com/\n",
      "luminatedata\n",
      "https://www.lyricfinancial.com/\n",
      "lyricfinancial\n",
      "https://www.mimecorp.com/\n",
      "mimecorp\n",
      "https://about.meta.com/\n",
      "about.meta\n",
      "https://miledigital.com/\n",
      "miledigital\n",
      "https://www.miquido.com/\n",
      "miquido\n",
      "https://www.themlc.com/\n",
      "themlc\n",
      "https://www.mnrk.com/\n",
      "mnrk\n",
      "https://www.monstercat.com/\n",
      "monstercat\n",
      "https://mozaic.io/\n",
      "mozaic\n",
      "http://www.mqa.co.uk/\n",
      "mqa.co\n",
      "http://mtheory.com/\n",
      "mtheory\n",
      "https://www.musicaudienceexchange.com/\n",
      "musicaudienceexchange\n",
      "https://www.mufi.app/\n",
      "mufi\n",
      "https://muserk.com/\n",
      "muserk\n",
      "https://music.ai/\n",
      "music\n",
      "http://themusic.fund\n",
      "themusic\n",
      "https://www.musicreports.com/\n",
      "musicreports\n",
      "http://themusicroyaltyco.uk/\n",
      "themusicroyaltyco\n",
      "http://www.music-story.com/\n",
      "music-story\n",
      "https://www.musixmatch.com/\n",
      "musixmatch\n",
      "https://mvdentertainment.com/\n",
      "mvdentertainment\n",
      "https://nash-audio.com/\n",
      "nash-audio\n",
      "http://nettwerk.com/\n",
      "nettwerk\n",
      "https://www.newburycomics.com/\n",
      "newburycomics\n",
      "https://www.nfhits.com/\n",
      "nfhits\n",
      "https://www.nielsen.com/\n",
      "nielsen\n",
      "http://www.ninety9lives.com/\n",
      "ninety9lives\n",
      "https://www.newburycomics.com/\n",
      "newburycomics\n",
      "http://www.nightbirde.co/\n",
      "nightbirde\n",
      "https://www.nivassoc.org/\n",
      "nivassoc\n",
      "https://www.nonesuch.com\n",
      "nonesuch\n",
      "https://nuemeta.com/\n",
      "nuemeta\n",
      "http://www.onemusicglobal.com/\n",
      "onemusicglobal\n",
      "https://onerpm.com/\n",
      "onerpm\n",
      "http://openplay.co/\n",
      "openplay\n",
      "https://www.theorchard.com/\n",
      "theorchard\n",
      "https://www.ocp.org/en-us\n",
      "ocp\n",
      "https://openonsunday.com/\n",
      "openonsunday\n",
      "https://opusmusicgroup.com/\n",
      "opusmusicgroup\n",
      "http://www.orfium.com/\n",
      "orfium\n",
      "https://www.pandora.com/\n",
      "pandora\n",
      "https://pex.com\n",
      "pex\n",
      " http://www.pigzebub.com\n",
      "pigzebub\n",
      "https://www.pitchwirestudio.com/\n",
      "pitchwirestudio\n",
      "https://platoon.ai/\n",
      "platoon\n",
      "https://www.prazor.com/\n",
      "prazor\n",
      "https://primarywave.com/\n",
      "primarywave\n",
      "https://propellersoundrecordings.com/\n",
      "propellersoundrecordings\n",
      "https://www.providentlabelgroup.com/\n",
      "providentlabelgroup\n",
      "https://www.prsformusic.com/\n",
      "prsformusic\n",
      "https://cirkay.com/\n",
      "cirkay\n",
      "https://www.qobuz.com/us-en/discover\n",
      "qobuz\n",
      "https://quarterlab.com/\n",
      "quarterlab\n",
      "https://rockpaperscissors.biz/\n",
      "rockpaperscissors\n",
      "https://www.rcarecords.com\n",
      "rcarecords\n",
      "https://www.realgonemusic.com/\n",
      "realgonemusic\n",
      "https://mes.rebeat.com/en/\n",
      "mes.rebeat\n",
      "https://www.redbullmediahouse.com/en\n",
      "redbullmediahouse\n",
      "https://redeyeworldwide.com/\n",
      "redeyeworldwide\n",
      "https://www.redreamfilms.com\n",
      "redreamfilms\n",
      "https://www.redstreetrecords.com/\n",
      "redstreetrecords\n",
      "https://www.reelmuzikwerks.com/\n",
      "reelmuzikwerks\n",
      "https://www.remedytech.io/\n",
      "remedytech\n",
      "https://renaissance.app/\n",
      "renaissance\n",
      "https://www.reprtoir.com/\n",
      "reprtoir\n",
      "http://www.republicrecords.com/\n",
      "republicrecords\n",
      "https://www.revelator.com/\n",
      "revelator\n",
      "https://www.rhino.com/\n",
      "rhino\n",
      "https://www.riaa.com/\n",
      "riaa\n",
      "https://www.rimaspublishing.com/\n",
      "rimaspublishing\n",
      "http://www.roadrunnerrecords.com/\n",
      "roadrunnerrecords\n",
      "https://royal.io/\n",
      "royal\n",
      "http://www.royaltyshare.com/\n",
      "royaltyshare\n",
      "https://www.royaltysolutionscorp.com/\n",
      "royaltysolutionscorp\n",
      "https://www.royfi.com/\n",
      "royfi\n",
      "https://rytebox.com/\n",
      "rytebox\n",
      "https://www.screenwavemedia.com/\n",
      "screenwavemedia\n",
      "https://secretlydistribution.com/\n",
      "secretlydistribution\n",
      "http://www.sesac.com/\n",
      "sesac\n",
      "https://www.session.id\n",
      "session\n",
      "https://www.shazam.com/\n",
      "shazam\n",
      "https://shinexmonitoring.com/\n",
      "shinexmonitoring\n",
      "https://www.sideways-media.com/\n",
      "sideways-media\n",
      "http://www.simplygrandmusic.com/\n",
      "simplygrandmusic\n",
      "https://singa.com/us\n",
      "singa\n",
      "https://singlemusic.com/\n",
      "singlemusic\n",
      "https://www.siriusxm.com/\n",
      "siriusxm\n",
      "https://folkways.si.edu/folkways-recordings/smithsonian\n",
      "folkways.si\n",
      "https://www.socan.com/\n",
      "socan\n",
      "https://songbox.com/\n",
      "songbox\n",
      "https://www.songclip.com/\n",
      "songclip\n",
      "https://www.songfluencer.com/\n",
      "songfluencer\n",
      "https://www.songkick.com/\n",
      "songkick\n",
      "https://www.songsleuth.io/#1\n",
      "songsleuth\n",
      "https://www.songtradr.com/\n",
      "songtradr\n",
      "https://www.songtrust.com\n",
      "songtrust\n",
      "https://sonosuite.com/en/\n",
      "sonosuite\n",
      "https://www.sonydadc.com/\n",
      "sonydadc\n",
      "https://www.sonymusic.com/\n",
      "sonymusic\n",
      "https://www.sonymusicmasterworks.com/news/\n",
      "sonymusicmasterworks\n",
      "https://www.sonymusicnashville.com/\n",
      "sonymusicnashville\n",
      "https://www.sonymusicpub.com\n",
      "sonymusicpub\n",
      "https://soundcloud.com/\n",
      "soundcloud\n",
      "https://www.soundexchange.com/\n",
      "soundexchange\n",
      "https://www.soundperformance.us/\n",
      "soundperformance\n",
      "https://soundroyalties.com/\n",
      "soundroyalties\n",
      "http://www.sparemusic.com/\n",
      "sparemusic\n",
      "https://www.spotify.com/us/\n",
      "spotify\n",
      "https://standtogethermusic.org/\n",
      "standtogethermusic\n",
      "https://linktr.ee/starlinermgmt\n",
      "linktr\n",
      "https://staxrecords.com/\n",
      "staxrecords\n",
      "https://www.stealthwrks.com/\n",
      "stealthwrks\n",
      "https://stemit.com/\n",
      "stemit\n",
      "https://www.stess.co/\n",
      "stess\n",
      "https://www.streamcut.com/\n",
      "streamcut\n",
      "https://www.subpop.com/\n",
      "subpop\n",
      "https://www.sunrecords.com\n",
      "sunrecords\n",
      "https://www.switchchord.com/\n",
      "switchchord\n",
      "https://symphonicdistribution.com/\n",
      "symphonicdistribution\n",
      "https://www.symphonyos.co/\n",
      "symphonyos\n",
      "https://www.synchtank.com/\n",
      "synchtank\n",
      "https://syneonline.com/\n",
      "syneonline\n",
      "https://syntaxcreative.com/\n",
      "syntaxcreative\n",
      "https://www.takwene.com/\n",
      "takwene\n",
      "https://www.thirdbridgecreative.com/\n",
      "thirdbridgecreative\n",
      "https://www.tnentertainment.com/\n",
      "tnentertainment\n",
      "https://www.tiktok.com/\n",
      "tiktok\n",
      "https://www.touchtunes.com/\n",
      "touchtunes\n",
      "https://tradablebits.com/\n",
      "tradablebits\n",
      "https://trevannatracks.com/\n",
      "trevannatracks\n",
      "https://www.troessexmusic.com/territory\n",
      "troessexmusic\n",
      "http://www.tropisounds.com/\n",
      "tropisounds\n",
      "https://www.tunecore.com/\n",
      "tunecore\n",
      "https://www.tunedglobal.com/\n",
      "tunedglobal\n",
      "https://www.twitch.tv/\n",
      "twitch\n",
      "https://unitedmasters.com/\n",
      "unitedmasters\n",
      "http://www.universalmusicenterprises.com/\n",
      "universalmusicenterprises\n",
      "https://www.universalmusica.com/\n",
      "universalmusica\n",
      "https://www.universalmusic.com\n",
      "universalmusic\n",
      "https://www.umusicpub.com/nashville\n",
      "umusicpub\n",
      "https://www.umusicpub.com/\n",
      "umusicpub\n",
      "https://www.umusicpub.com/nashville\n",
      "umusicpub\n",
      "https://www.vanheusenmusic.com/\n",
      "vanheusenmusic\n",
      "https://verifi.media/\n",
      "verifi\n",
      "http://www.vevasound.com/\n",
      "vevasound\n",
      "https://hq.vevo.com/\n",
      "hq.vevo\n",
      "https://www.viberate.com/\n",
      "viberate\n",
      "http://www.vinylren.com\n",
      "vinylren\n",
      "https://www.virgin.com/about-virgin/latest/introducing-virgin-music-label-and-artist-services\n",
      "virgin\n",
      "https://www.vistex.com/\n",
      "vistex\n",
      "https://vinylkey.com/\n",
      "vinylkey\n",
      "https://voltcreative.com/\n",
      "voltcreative\n",
      "https://vydia.com/\n",
      "vydia\n",
      "https://www.thevinylab.com/\n",
      "thevinylab\n",
      "https://www.vprecords.com/\n",
      "vprecords\n",
      "https://disneymusic.disney.com/\n",
      "disneymusic.disney\n",
      "http://www.warnerchappell.com/\n",
      "warnerchappell\n",
      "http://www.warnermusic.ca/\n",
      "warnermusic\n",
      "https://www.wmg.com/\n",
      "wmg\n",
      "http://www.warnermusicnashville.com/\n",
      "warnermusicnashville\n",
      "http://www.warnerrecords.com\n",
      "warnerrecords\n",
      "https://www.watertower-music.com/index.php\n",
      "watertower-music\n",
      "https://axis.wmg.com/login/\n",
      "axis.wmg\n",
      "https://www.wealthdistro.com/\n",
      "wealthdistro\n",
      "http://www.welldunn.org/\n",
      "welldunn\n",
      "https://www.belmont.edu/\n",
      "belmont\n",
      "https://www.berklee.edu/\n",
      "berklee\n",
      "https://www.bgsu.edu/musical-arts.html\n",
      "bgsu\n",
      "https://www.colum.edu/\n",
      "colum\n",
      "https://www.csun.edu/\n",
      "csun\n",
      "https://www.ftc.edu/\n",
      "ftc\n",
      "https://hello.fullsail.edu/\n",
      "hello.fullsail\n",
      "https://www.loyno.edu/\n",
      "loyno\n",
      "https://www.monmouth.edu/\n",
      "monmouth\n",
      "https://www.murraystate.edu/\n",
      "murraystate\n",
      "https://www.nyu.edu/\n",
      "nyu\n",
      "https://www.northeastern.edu/\n",
      "northeastern\n",
      "https://www.temple.edu/\n",
      "temple\n",
      "http://www.tnstate.edu/music\n",
      "tnstate\n",
      "https://www.torontomu.ca/\n",
      "torontomu\n",
      "https://www.ucla.edu/\n",
      "ucla\n",
      "https://www.uga.edu/\n",
      "uga\n",
      "https://welcome.miami.edu/\n",
      "welcome.miami\n",
      "https://music.unt.edu/\n",
      "music.unt\n",
      "https://www.wpunj.edu/\n",
      "wpunj\n"
     ]
    }
   ],
   "source": [
    "site_list = []\n",
    "for site in url_list:\n",
    "    print(site)\n",
    "    extracted = extract_domain_name_compact(site)\n",
    "    print(extracted)\n",
    "    site_list.append(extracted)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "51f0f4b8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
